The necessity of reducing dimensions in a large data set serves different purposes in the problems we face in data science. 
Now follows a summary of the different applications where we can utilise the methods of \acrlong{dr}.
Finally, we will conclude how despite their disparities, the various application methods come together in our goal of finding tractable and resource-conscious solutions.
\vspace{-5mm}

\paragraph{Computational impact}

The most intuitive benefit is certainly its impact on computational performance.
Having fewer features reduces our required storage space and allows our learning algorithms to run much faster. \cite{PythonMachineLearningCh1}
\vspace{-5mm}

\paragraph{Feature engineering}

Our models will only be capable of producing relevant results if the features we supply it with are also relevant. \cite{HandsOnMLCh1}
\emph{Feature extraction} combines many semantically related features into few, which are found through \gls{dr}.
This helps us to reduce the number of features while retaining as much information as possible from the originating data set.
This is not to be confused with \emph{feature selection}, which does not benefit from \gls{dr} and is therefore not considered in this work.
\vspace{-5mm}

\paragraph{Data visualisation}

Reducing dimensions can also help us to visually represent data in an intuitive fashion.
Even picturing a relatively basic 4-dimensional hypercube is incredibly difficult to make sense of.
Actively recalling this information is not only important for our own understanding of data. It is even more important when sharing our observations and ideas with other data scientists and essential when communicating our conclusions to people in an interdisciplinary environment. \cite{PythonMachineLearningCh5}
\vspace{-5mm}

% \paragraph{Traceability}

% Having discussed the importance of understanding the problem and its solution earlier in section \ref{curseOfDimensionality}, it is often important to be able to comprehend the prediction a model makes instead of trusting a black box model. 
% Reducing the dimensions and understanding the given data helps us to alleviate this issue.
% \vspace{-5mm}

\paragraph{Conclusion}

Considering the computational impact helps us to narrow down a complex task into a feasible scope to find resource-conscious solutions.
Feature extraction helps us to remove dimensions coupled with an increase in predictive performance right off the bat.
This helps us to visualise the data and to be able to understand and trace its predictions.
Combined, this helps us to build a tractable solution to our problem.
