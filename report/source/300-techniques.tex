Being acquainted with the premise, we now possess a coarse understanding of the relevant mathematical scaffold required to understand dimensionality reduction.
Additionally, we became aware of the potential pitfalls that hold true to the subject in general.
Therefore, we are now ready to take a closer look on how to categorise and associate various techniques.


\subsection{Linear vs. non-linear problems}

The utmost category, used to differentiate between various techniques is fortunately mutually exclusive to any method.
Thanks to this characteristic, we can distinctively classify and associate a given technique.
This allocation, contrary to the following classifications, is the one that considers the domain of the problem.
Thus, it requires special attention by its users since it can easily be used erroneously.
We need to understand the gross patterns in the data set we need to operate on.

The below figure \ref{fig:linearvsnonlinearproblems} illustrates and contrasts the general patterns that we need to identify.\vspace*{4mm}

\renewcommand{\tikzscale}{1.25}
\begin{figure}[h]
	\begin{subfigure}{0.48\textwidth}
	    \caption{Linear problems}
		\input{source/301-linear_problem.tex}
	    \label{subfig:linearproblems}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
	    \caption{Non-linear problems}
		\input{source/302-non_linear_problem.tex}
	    \label{subfig:nonlinearproblems}
	\end{subfigure}
\caption{Linear vs. non-linear problems.}
\label{fig:linearvsnonlinearproblems}
\end{figure}

As we can see in figure \ref{subfig:linearproblems}, a linear problem is given when we are able to identify a straight line, or any \gls{hyperplane}, to split the data into unambiguous subsets.
Intuitively and, as we will later examine, factually, these types of problem are comparably easy to solve.

A non-linear problem, as pictured in figure \ref{subfig:nonlinearproblems}, already looks more complex on the first glance.
And indeed, we will illuminate the solution approaches to this scenario and elaborate on its comportment.

\clearpage

\subsection{Projection vs. manifold learning}
% taken from ch 8 in g√©ron's book

In this section, we will compare the general solution approaches available which can be utilised to solve both linear as well as non-linear problems. \cite{HandsOnMLCh8}

\todo{Not quite true, revisit this. \cite{Lee2007NonlinearDR} cite this.}

\renewcommand{\tikzscale}{0.33}
\begin{wrapfigure}[13]{r}{0.62\textwidth}
	\vspace*{-8mm}
	\centering
	\input{source/303-projection_example.tex}
	\captionsetup{justification=centering}
	\caption{Simple example of a projection}
    \label{fig:projectionExample}
\end{wrapfigure}

\paragraph{Projection} In contrast, this is the trivial concept of the two. The idea is to project the data points onto a \gls{hyperplane} which summarises the data with as little information loss as possible.

Figure \ref{fig:projectionExample} illustrates this in a simple example.
As we can observe, when we pick the right \gls{hyperplane}, such as the x axis in the example, we lose far fewer information than if we would have picked the y axis.


\paragraph{Manifold learning} This concept is significantly more difficult to get a hold of.
Significant breakthroughs \cite{ma2012manifold} in this field were accomplished in the year 2000 in the significant and commonly cited paper \emph{A global geometric framework for nonlinear dimensionality reduction}. \cite{tenenbaum2000global}
To understand the basic idea, we will demonstrate its behaviour and briefly dive into the mathematical details using the popular swiss roll data set pictured in figure \ref{fig:swissrollfull}.

\begin{wrapfigure}[13]{l}{0.42\textwidth}
	\centering
	\includegraphics[width=0.4\textwidth]{external_content/graphs/swiss_roll.png}
	\captionsetup{justification=centering}
	\caption{Swiss Roll generated from scikit-learn \cite{scikit-learn}}
    \label{fig:swissrollfull}
\end{wrapfigure}

Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod
tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,
quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non
proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\todo{Finish this section}

\clearpage

\ 

\begin{center}
	\textit{placeholder for swiss roll demo}
\end{center}


\clearpage


\subsection{Linear techniques}

We will take a look at \gls{pca} \& \gls{lda}.



\subsubsection{Linear Discriminant Analysis}

\begin{itemize}
	\item First approaches made in 1936. \cite{fisher1936use}
	\item Various solvers
	\begin{itemize}
		\item \textbf{Eigenvalue Decomposition}
		\item Runs in $\bigo{N^3}$ according to \cite{cai2008training}
	\end{itemize}
	\begin{itemize}
		\item \textbf{LSQR}
		\item Runs in $\bigo{N^2}$ according to \cite{di2013new}
	\end{itemize}
		\begin{itemize}
		\item \textbf{SVD}
		\item Runs in $\bigo{}$ according to %\cite{di2013new}
	\end{itemize}
\end{itemize}

$$\bigo{TBA}$$

\clearpage


\begin{center}
	\textit{Not sure what comes here, but most certainly going to be two pages}
\end{center}

\clearpage





\subsubsection{Principal Component Analysis}

\begin{itemize}
	\item First approaches made in 1901 using simple projections. \cite{pearson1901liii}
	\item Primarily used for feature extraction \cite{PythonMachineLearningCh5}
	\item Unsupervised method \cite{PythonMachineLearningCh5}
	\item Is an eigenvector problem \cite{MultilinearSubspaceLearningCh2}
	\item scikit-learn implemented this version \cite{minka2000automatic}
\end{itemize}

\clearpage

\paragraph{full \gls{svd}}

According to \cite{wright2001large}

$$\bigo{N^3}$$

\clearpage

\paragraph{\gls{arpack}}

According to \cite{wright2001large}

$$\bigo{N^2}$$

\clearpage

\paragraph{randomised}

According to this \cite{HandsOnMLCh8}

$$\bigo{d^3}$$


\clearpage

\paragraph{Conclusion}



\clearpage




\subsection{Non-linear techniques}

Mention other non-linear techniques that exist.





\subsubsection{Kernel Principal Component Analysis}

\begin{itemize}
	\item First introduced by \cite{scholkopf1998nonlinear}
	\item According to this ...%\cite{}
\end{itemize}

$$\bigo{TBA}$$

\clearpage

\begin{center}
	\textit{Not sure what comes here, but most certainly going to be two pages}
\end{center}

\clearpage







\subsubsection{Locally Linear Embedding}

\begin{itemize}
	\item First introduced by \cite{roweis2000nonlinear}
	\item According to this \cite{DRUnsupervisedNearestNeighbors}
\end{itemize}

$$\bigo{N^2}$$

\clearpage

\begin{center}
	\textit{Not sure what comes here, but most certainly going to be two pages}
\end{center}

\clearpage




\subsubsection{Isomap Embedding}

\begin{itemize}
	\item \Gls{lle} first introduced by \cite{tenenbaum2000global}
	\item Tries to preserve the geodesic distances between the instances \cite{HandsOnMLCh8}
	\item According to this \cite{DRUnsupervisedNearestNeighbors}
\end{itemize}

$$\bigo{N^2 \log N}$$

\clearpage

\begin{center}
	\textit{Not sure what comes here, but most certainly going to be two pages}
\end{center}

\clearpage



\subsubsection{t-SNE}

\begin{itemize}
	\item \gls{tsne} first introduced by \cite{van2008visualizing}
	\item Primarily used to visualise datasets
	\item Tries to preserve the proximity between instances \cite{HandsOnMLCh8}
	\item According to this \cite{van2014accelerating}
\end{itemize}

$$\bigo{N \log N}$$

\clearpage

\begin{center}
	\textit{Not sure what comes here, but most certainly going to be two pages}
\end{center}

\clearpage
