\subsubsection{Introduction}

After having explored the origins and early adaptations of \gls{pca}, it is time to introduce the approach used to compute \gls{pca} nowadays: using the \acrlong{svd}.
For this, we will take a look at how \gls{pca} is implemented in \gls{scikit} and quickly observe that all variations rely on \gls{svd} internally.\bigskip


In a sense, the \gls{svd} generalises the ideas behind the fast Fourier transform, which for example is used in media compression algorithms. 
Whereas the fast Fourier transform provides a generic approximation for a data set, the \gls{svd} can be seen as a data-driven technique.
The \gls{svd} adapts itself to the data set as a whole.
Another major different is that the decomposition of the \gls{svd} can be reversed by approximation.
\bigskip


Generally speaking, the goal of \gls{svd} is to decompose a matrix into its components which are sorted by their most statistically descriptive factors.
The primary advantage of the \gls{svd} is the efficiency and versatility of applications it can be used in.

The efficiency is key for ever growing data sets that are available and collected in modern times.
The versatility is a neat byproduct and conveyor of its popularity, because the different flavours \gls{svd} comes in and provides vast combinations of benefits and drawbacks.
These characteristics can furthermore be tailored to a specific problem in order to either optimise it or to make it feasible to begin with.

Additionally, one major advantage from \gls{svd} over other approaches, such as the standard approach from section \ref{section:standardApproach}, is that the \gls{svd} provides a robust matrix which is guaranteed to exist \cite{brunton2019data}.\bigskip



To get an idea of how \gls{pca} and \gls{svd} are related to each other, a brief description is that \gls{pca} is a statistical application of an \gls{svd} \cite{brunton2019data}.
\Gls{pca} performs one additional task in contrast to a classical \gls{svd}: the data is centered prior to its application.
The \gls{svd} in general provides a hierarchical representation of the data inside a new matrix which is defined by dominant correlations within a given data set.
\bigskip



To end our introduction, it is interesting to know that the \gls{svd} provides the basis for other statistical techniques.
These include the Karhunen-Loève transform (KLT), empirical orthogonal functions (EOFs) and canonical correlation analysis (CCA).
As these techniques are not closely related to \gls{pca}, none of these will be refered to in this work.




% \item Important as modern PCA algorithms are basically a statistical application of \gls{svd}
% \item Modern PCA is basically an applied version of SVD on a centered data matrix, decomposed into its most statistically descriptive factors
% \item SVD generalizes the concept of the fast Fourier transform: FFT on generic basis, SVD is tailored to the specific data
% \item Matrix factorization
% \item Provides a numerically stable matrix decomposition which is guaranteed to exist (unlike the eigendecomposition) \cite{brunton2019data}: robust and efficient
% \item Basis for other statistical techniques: Karhunen-Loève transform (KLT) or empirical orthogonal functions (EOFs), canonical correlation analysis (CCA)

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsubsection{History}

% \begin{itemize}
% 	\item Historical overview by \cite{stewart1993early}
% 	\item Foundation by
% 	\begin{itemize}
% 		\item First independent approaches in Italy \cite{beltrami1873sulle}
% 		\item Second independent approaches in France \cite{Jordan1874}	
% 	\end{itemize}
% 	\item Modern work on computational stability and effiency \cite{brunton2019data}
% 	\begin{itemize}
% 		\item Golub mentioned in Brutons work and Jolliffe's recap: \cite{golub1965calculating} \& \cite{golub1971singular}
% 	\end{itemize}
% \end{itemize}

% \clearpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{General idea}

The general idea behind the \gls{svd} is to analyse large matrices which represent data sets such as  $A \in \mathbb{R}^{n \times m}$ represented in \reff{svdMatrixShape}:

\begin{equation}
	\label{formula:svdMatrixShape}
	A = 
	\begin{bmatrix}
		\mid & \mid & & \mid\\
		x_1 & x_2 & \cdots & x_m\\
		\mid & \mid & & \mid\\
	\end{bmatrix}
\end{equation}

\vspace*{4mm}

The matrix is then decomposed into three parts:

\vspace{-6mm}
\begin{align}
	\label{formula:svdBasic}
	A = U \Sigma V^T
\end{align}

$U \in \mathbb{R}^{n \times n}$ and $V \in \mathbb{R}^{m \times m}$ are unitary matrices with orthogonal columns.
The colums of $U$ are called the \emph{left-singular vectors} of $A$ and $V$ are the \emph{right-singular vectors} of $A$.
$\Sigma \in \mathbb{R}^{n \times m}$ contains non-negative entries on the diagonal with zeros everywhere else.
The non-negative entires in $\Sigma$ are called the \emph{singular values} and are ordered from largest to slowest importance from 1 to $m$ \cite{brunton2019data}.
\bigskip


$U$ and $V$ being square matrices is achieved thanks to computing the correlation matrix from both the rows and the columns of a matrix $A$.

The columns of $U$ capture the eigenvalues of the correlation matrix of $AA^T$.
This represents the correlation between the columns of $A$.
The columns of $V$ capture the eigenvalues of the correlation matrix of $A^TA$.
This represents the correlation between the rows of $A$ \cite{brunton2019data}.  % section 1.3, page 13
\bigskip


For the next paragraph, we are assuming that $n>>m$\footnote{$n$ is significantly larger than $m$}.
Now that we know what the decomposed matrices contain, we quickly realise the immense computational power that is required to construct the matrix $AA^T$.
Let alone solve the eigenvalue problem to this matrix.

Thankfully, \gls{svd} delivers a few different approches to either efficiently compute the eigenvalues or to accurately approximate the \gls{singular} values.
This is achived using sampling methods or by the help of randonmess and probability.
These methods significantly reduce the required computational resources.
In order to contrast this, the following methods that will be considered will compare the runtime complexity.



\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
