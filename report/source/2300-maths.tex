
To round up the theoretical premises required for this topic, we will become aware of the boundaries of intuitive mathematical concepts which result in highly counter-intuitive behaviours in high dimensional space, as well as their practical solution approaches.

\subsubsection{Euclidean distance \& sparse matrices}

An important aspect which is frequently utilised in various machine learning methods is to evaluate the euclidean distance between two points in a high-dimensional space.
While the concept is simple to understand and illustrate in two or three dimensions, its behaviour in a high-dimensional space changes dramatically and it becomes heavily counter-intuitive to get a hold off.

Most notably, if you pick two random points in a unit hypercube, the higher the dimensions, the higher the average distance between these two points will be \cite{HandsOnMLCh8}.
This implies that, the higher the dimensionality of a dataset, the higher the chance of overfitting.
In this scenario, the matrices which represent a high-dimensional dataset are called sparse matrices.

\vspace{2mm}



\subsubsection{Eigenvalues and eigenvectors}

\begin{center}
	\textit{Quickly recapitulate eigenvectors and explain why they are relevant}
\end{center}


