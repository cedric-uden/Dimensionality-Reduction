
To round up the theoretical premises required for this topic, we will become aware of the boundaries of intuitive mathematical concepts which result in highly counter-intuitive behaviours in high dimensional space, as well as their practical solution approaches.

\subsubsection{Euclidean distance \& sparse matrices}

An important aspect which is frequently utilised in various machine learning methods is to evaluate the Euclidean distance between two points in a high-dimensional space.
While the concept is simple to understand and illustrate in two or three dimensions, its behaviour in a high-dimensional space changes dramatically and it becomes heavily counter-intuitive to get a hold off.

Most notably, if you pick two random points in a unit hypercube, the higher the dimensions, the higher the average distance between these two points will be \cite{HandsOnMLCh8}.
This implies that, the higher the dimensionality of a data set, the higher the chance of overfitting.
In this scenario, the matrices which represent a high-dimensional data set are called sparse matrices.

\vspace{2mm}



\subsubsection{Eigenvalues and eigenvectors}

The theories behind eigenvalues and eigenvectors are utilised extensively in \acrlong{dr}.
Due to this, we will quickly recall the key ideas behind the eigenvalue equation $A \overrightarrow{x} = \lambda \overrightarrow{x}$:


\begin{itemize}
	\item Eigenvalues and eigenvectors only apply to squared matrices.
	\item When searching for an eigenvalue $\lambda$, we aim to subtract $\lambda$ along the main diagonal of a matrix $A$ such that $A - \lambda I$ is \gls{singular} ($I$ being the identity matrix) \cite{Strang2005tn}.
	\item The entire factorisation how to conclude to $A - \lambda I$ looks as following:
	\begin{align}
		\label{formula:eigenONE}
		A \overrightarrow{x} &= \lambda \overrightarrow{x} 
		\\
		%
		\label{formula:eigenTWO}
		A \overrightarrow{x} &= (\lambda I) \overrightarrow{x}
		\\
		%
		\label{formula:eigenTHREE}
		A \overrightarrow{x} - (\lambda I) \overrightarrow{x} &= 0
		\\
		%
		\label{formula:eigenFOUR}
		(A - \lambda  I) \overrightarrow{x} &= 0
		%
	\end{align}
	\item \reff{eigenTWO} translates the scalar-vector multiplication into a matrix-vector multiplication
	\item $\overrightarrow{x} = \overrightarrow{0}$ is not expressive and therefore not considered
\end{itemize}

% \begin{itemize}
% 	\item Only square matrices have eigenvectors and eigenvalues
% 	\item We will make heavy use of transposing matrices
% 	\item Identity matrix, Properties of diagonal matrices, General formula
% 	\begin{align}
% 		\label{formula:one}
% 		A \cdot x &= \lambda \cdot x 
% 		\\
% 		%
% 		\label{formula:two}
% 		A \cdot x &= \lambda \cdot x \cdot I
% 		\\
% 		%
% 		\label{formula:three}
% 		(A - \lambda \cdot I)\cdot x &= 0
% 		%
% 	\end{align}
% \end{itemize}

% \reff{one}

% \reff{two}

% \reff{three}
