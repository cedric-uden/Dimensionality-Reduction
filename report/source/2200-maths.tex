
To round up the theoretical premises required for this topic, we will become aware of the boundaries of intuitive mathematical concepts which result in highly counter-intuitive behaviours in high dimensional space, as well as their practical solution approaches.

\subsubsection{Euclidean distance \& sparse matrices}

An important aspect which is frequently utilised in various machine learning methods is to evaluate the euclidean distance between two points in a high-dimensional space.
While the concept is simple to understand and illustrate in two or three dimensions, its behaviour in a high-dimensional space changes dramatically and it becomes heavily counter-intuitive to get a hold off.

Most notably, if you pick two random points in a unit hypercube, the higher the dimensions, the higher the average distance between these two points will be \cite{HandsOnMLCh8}.
This implies that, the higher the dimensionality of a dataset, the higher the chance of overfitting.
In this scenario, the matrices which represent a high-dimensional dataset are called sparse matrices.

\vspace{2mm}



\subsubsection{Eigenvalues and eigenvectors}

Linear algebra knowledge is required for this paper.
This would have to be a book in its own to recapitulate the entirety of the matter.
The most important aspects are:

\begin{itemize}
	\item Only square matrices have eigenvectors and eigenvalues
	\item We will make heavy use of transposing matrices
	\item Identity matrix
	\item Properties of diagonal matrices
	\item General formula
	\begin{align}
		\label{formula:one}
		A \cdot x &= \lambda \cdot x 
		\\
		%
		\label{formula:two}
		A \cdot x &= \lambda \cdot x \cdot I
		\\
		%
		\label{formula:three}
		(A - \lambda \cdot I)\cdot x &= 0
		%
	\end{align}
\end{itemize}

\reff{one}

\reff{two}

\reff{three}
