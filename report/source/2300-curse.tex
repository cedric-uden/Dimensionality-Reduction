The concept of the \emph{curse of dimensionality} was first introduced by Richard Bellman in 1957 \cite{DynProg}.
His book, \emph{Dynamic Programming}, explains a method he has developed to explore more efficient solutions to counteract the increasing complexity in problems facing our day-to-day lives. 
The range of domains where applicable is vast and even covers problems that were impossible to foresee for Richard Bellman.
Most notable to us, this includes data science in the 21st century and falls right into the realm of \acrlong{dr}.
\bigskip

Bellman observed that, when considering a larger set of variables, even simple and well-understood problems such as determining the maximum of a given function becomes worrisome.
Large data sets face a variety of difficulties of both obvious and subtle nature.

The obvious issues primarily consist of a finite number of computational resources available, especially back in the 1950s. 
Having access to the computer systems we have today, they could certainly be considered a computational nirvana for any scientists and mathematicians 65 years ago.
While Bellman fantasised about such possibilities, he proactively considered them and accurately thought of more subtle problems that could potentially arise.
His observations were spot on and are an important factor in why his theories are still as relevant as ever nowadays.

\emph{The problem is not to be considered solved in the mathematical sense until the structure of the optimal policy is understood} \cite{DynProg}.
This quote from Richard Bellman eloquently summarises the problem at hand.  
While it appears plausible, that more quantitative measurements would yield more accurate predictions, our extremely complicated world often misleads us.
This results in ourselves neither being able to rigorously understand the problem, nor to improve our predictions of challenges we are unable to analyse.
We need to walk a narrow path between the \emph{Pitfalls of Oversimplification and the Morass of Overcomplication} \mycite{DynProg}.
\bigskip

With the premise being described by Bellman in 1957, we can conclude that many features do not only heavily impede model training but can additionally result in worse solutions \cite{HandsOnMLCh8}.
Simultaneously when considering the performance and traceability aspects.
