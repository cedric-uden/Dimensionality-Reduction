This section on \gls{pca} will be the main constituent of this paper.
We will look at the linear classifier that is offered using \gls{pca} and get a feeling of its strengths and weaknesses.
\bigskip


\Gls{pca} is one of the oldest and simultaneously most widespread methods used to reduce the dimensionality of a large data set.
We will first look at the vast number of important initial ideas and milestones during its development.

After getting a brief outline of the subject, we will have a look at some of the models used to determine \glspl{pc}: first a more traditional approach, combining various theorems in linear algebra. 
Finally, we will examine the \gls{svd} as a tool to decompose a matrix.

\Gls{svd} is the more recent approach which is only feasible thanks to the modern computation power available in recent years.
This approach will be accompanied by exploring the implementation of \gls{pca} in \gls{scikit}.
\Gls{scikit} is a project initialised by Google in 2007 and amongst the most popular repositories in the field of data science \cite{scikit-learn}.
\bigskip


An important contribution to its popularity is surely the wide range of applications \gls{pca} can be applied to.
The areas are nearly endless and continuously growing --- agriculture, economics, genetics, oceanography, psychology --- just to name a few.
The \emph{Web of Science} identified over 2000 articles published in 1999 and 2000 alone that related to \gls{pca} \cite{Jolliffe2002book}.
And with modern computational resources being an important driver in its development, the collection is expanding to this day.
\bigskip


The key idea behind \gls{pca} is to increase interpretability while at the same time minimising information loss. 
This is possible thanks to creating new uncorrelated variables that sequentially maximise their variance.
The methods mentioned above, the traditional approach and the one using \gls{svd}, differ significantly. 
These and many more are tailored to the various data types and structures of data sets in each problem \cite{jolliffe2016principal}.

% \item Primarily used for feature extraction \cite{PythonMachineLearningCh5}
% \item Used in an abundance of fields... \cite{Jolliffe2002book} (Section 1.2 page 9)
% \item Is an eigenvector problem \cite{MultilinearSubspaceLearningCh2}
% \item Will try our best to give a quick overview
% \item Very old and relevant topic
% \item Unsupervised method \cite{PythonMachineLearningCh5}
% \item Will take a look at the standard approach and the modern way (svd)
% \item scikit implements this \cite{tipping1999mixtures}
% \item Maximize variability / statistical information \cite{jolliffe2016principal}
% \item Two approaches: eigenproblem or from SVD \cite{jolliffe2016principal}
% \item requires no distributional assumptions and is therefore on numerical data of various types \cite{jolliffe2016principal}


% \subsubsection{Intro}
% \clearpage

% \subsubsection{History}
% \clearpage

% \subsubsection{PCA in detail}
% Scikit implementation \cite{Tipping:2006va}
% \ \clearpage
% \ \clearpage
% \ \clearpage
% \ \clearpage

% \subsubsection{Minka's method to guess intrinsic dimensionality}
% \ \clearpage
% \ \clearpage

% \subsubsection{introduction to svd}
% \ \clearpage
% \ \clearpage

% \subsubsection{svd: full method}
% \ \clearpage
% \ \clearpage

% \subsubsection{svd: ARPACK}
% \ \clearpage
% \ \clearpage

% \subsubsection{svd: randomized}
% \ \clearpage
% \ \clearpage

% \paragraph{full \gls{svd}} \label{svd}

% According to \cite{wright2001large}

% $$\bigo{N^3}$$

% \clearpage

% \paragraph{\gls{arpack}}

% According to \cite{wright2001large}

% $$\bigo{N^2}$$

% \clearpage

% \paragraph{randomised}

% According to this \cite{HandsOnMLCh8}

% $$\bigo{d^3}$$


% \clearpage

% \paragraph{Conclusion}
