\begin{itemize}
	\item First approaches made in 1901 using simple projections. \cite{pearson1901liii}
	\vspace{-2mm}
	\item Primarily used for feature extraction \cite{PythonMachineLearningCh5}
	\vspace{-2mm}
	\item Unsupervised method \cite{PythonMachineLearningCh5}
	\vspace{-2mm}
	\item Is an eigenvector problem \cite{MultilinearSubspaceLearningCh2}
	\vspace{-2mm}
	\item scikit-learn implemented this version \cite{minka2000automatic} to guess the output dimesionality. \cite{halko2009finding} is used to decompose the input matrix.
	\vspace{-2mm}
	\item \gls{pca} assumes that the data is centered around the origin \cite{HandsOnMLCh8}. scikit-learn takes care of that.
	\vspace{-2mm}
	\item \gls{svd} is a matrix factorization technique \cite{HandsOnMLCh8}
\end{itemize}




\subsubsection{Projection}

% taken from ch 8 in g√©ron's book

In this section, we will compare the general solution approaches available which can be utilised to solve both linear as well as non-linear problems. \cite{HandsOnMLCh8}

\todo{Not quite true, revisit this. \cite{Lee2007NonlinearDR} cite this.}

\renewcommand{\tikzscale}{0.33}
\begin{wrapfigure}[13]{r}{0.62\textwidth}
	\vspace*{-8mm}
	\centering
	\input{source/3101-projection_example.tex}
	\captionsetup{justification=centering}
	\caption{Simple example of a projection}
    \label{fig:projectionExample}
\end{wrapfigure}

\paragraph{Projection} In contrast, this is the trivial concept of the two. The idea is to project the data points onto a \gls{hyperplane} which summarises the data with as little information loss as possible.

Figure \ref{fig:projectionExample} illustrates this in a simple example.
As we can observe, when we pick the right \gls{hyperplane}, such as the x axis in the example, we lose far fewer information than if we would have picked the y axis.




% \subsubsection{Intro}
% \clearpage

% \subsubsection{History}
% \clearpage

% \subsubsection{PCA in detail}
% Scikit implementation \cite{Tipping:2006va}
% \ \clearpage
% \ \clearpage
% \ \clearpage
% \ \clearpage

% \subsubsection{Minka's method to guess intrinsic dimensionality}
% \ \clearpage
% \ \clearpage

% \subsubsection{introduction to svd}
% \ \clearpage
% \ \clearpage

% \subsubsection{svd: full method}
% \ \clearpage
% \ \clearpage

% \subsubsection{svd: ARPACK}
% \ \clearpage
% \ \clearpage

% \subsubsection{svd: randomized}
% \ \clearpage
% \ \clearpage

% \paragraph{full \gls{svd}} \label{svd}

% According to \cite{wright2001large}

% $$\bigo{N^3}$$

% \clearpage

% \paragraph{\gls{arpack}}

% According to \cite{wright2001large}

% $$\bigo{N^2}$$

% \clearpage

% \paragraph{randomised}

% According to this \cite{HandsOnMLCh8}

% $$\bigo{d^3}$$


% \clearpage

% \paragraph{Conclusion}
