\begin{itemize}
	\item Primarily used for feature extraction \cite{PythonMachineLearningCh5}
	\item Used in an abundance of fields... \cite{Jolliffe2002book} (Section 1.2 page 9)
	\item Is an eigenvector problem \cite{MultilinearSubspaceLearningCh2}
	\item Will try our best to give a quick overview
	\item Very old and relevant topic
	\item Unsupervised method \cite{PythonMachineLearningCh5}
	\item Will take a look at the standard approach and the modern way (svd)
	\item scikit implements this \cite{tipping1999mixtures}
	\item Maximize variability / statistical information \cite{jolliffe2016principal}
	\item Two approaches: eigenproblem or from SVD \cite{jolliffe2016principal}
	\item requires no distributional assumptions and is therefore on numerical data of various types \cite{jolliffe2016principal}
\end{itemize}



% \subsubsection{Intro}
% \clearpage

% \subsubsection{History}
% \clearpage

% \subsubsection{PCA in detail}
% Scikit implementation \cite{Tipping:2006va}
% \ \clearpage
% \ \clearpage
% \ \clearpage
% \ \clearpage

% \subsubsection{Minka's method to guess intrinsic dimensionality}
% \ \clearpage
% \ \clearpage

% \subsubsection{introduction to svd}
% \ \clearpage
% \ \clearpage

% \subsubsection{svd: full method}
% \ \clearpage
% \ \clearpage

% \subsubsection{svd: ARPACK}
% \ \clearpage
% \ \clearpage

% \subsubsection{svd: randomized}
% \ \clearpage
% \ \clearpage

% \paragraph{full \gls{svd}} \label{svd}

% According to \cite{wright2001large}

% $$\bigo{N^3}$$

% \clearpage

% \paragraph{\gls{arpack}}

% According to \cite{wright2001large}

% $$\bigo{N^2}$$

% \clearpage

% \paragraph{randomised}

% According to this \cite{HandsOnMLCh8}

% $$\bigo{d^3}$$


% \clearpage

% \paragraph{Conclusion}
