Now that we are familiar with the ideas behind \gls{svd} and aware of the connection between \gls{pca} and \gls{svd}, it is time to see how \gls{pca} is implemented in \gls{scikit}.
We will explore three different possible solvers to decompose large matrices and evaluate their strengths and weaknesses.
\bigskip



In order to select the right solver for the problem, which are all based on \gls{svd}, \gls{scikit}'s \texttt{PCA} class provides the keyword argument \texttt{`solver`} to set one of the following strings: \texttt{`auto`, `full`, `randomized`, `arpack`}.
All of these can be set manually, or if set to \texttt{`auto`}, scikit will choose the one it deems fitting.
\bigskip

The current implementation looks as following:%
\footnote{\href{\scikitPCAvIxOxI{_pca}}{\texttt{sklearn.decomposition.PCA} (v. 1.0.1)}}






\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=447,
]
if max(X.shape) <= 500 or n_components == "mle":
    self._fit_svd_solver = "full"
elif n_components >= 1 and n_components < 0.8 * min(X.shape):
    self._fit_svd_solver = "randomized"
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
# [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=scikit implementation to select\\an adequate PCA solver, 
label=lst:autoPCA,
firstnumber=452,
]
else:
    self._fit_svd_solver = "full"
\end{lstlisting}


\medskip\noindent
As we can see in listing \ref{lst:autoPCA} above, the choice of the adequate solver is primarily dependent on the parameter \texttt{`n\_components`}.
This keyword allows us to explicitly declare the number of \acrlongpl{pc} to keep.

\texttt{`n\_components`} accepts either a \texttt{float}, which will select the number of components such that the quota of variance desired is reached; an \texttt{int}, which will select as many principal components; or the string \texttt{`mle`}, which estimates a reasonable number of \gls{pc} to keep and will be discussed in greater detail in the upcoming section \ref{sec:mle}.
\bigskip


If the number of \gls{pc} to choose is either not declared or a quota of variance is desired, the solver will always choose \texttt{`full`}.
If a set number of \gls{pc} is requested, the solver will be set on \texttt{`randomized`} as long as the number of \gls{pc} is lower than 80\% of the smallest dimension of the data set.


\ \clearpage


\subsubsection{Full SVD}

To get a detailed overview of the different solvers, we will start with the default value which is simultaneously the most prominent one.
Following \gls{scikit}'s documentation, we know that \cite{scikit-learn} implements the standard LAPACK solver via \texttt{scipy.linalg.svd}.
\bigskip

LAPACK is an acronym for Linear Algebra Package and is a library of Fortran 77 subroutines which solves most commonly occuring problems in linear algebra \cite{anderson1999lapack}.
It is designed to be maximally efficient on the whole spectrum of modern computers, with a particular focus on high-performance computers.

In order to understand the parameters and interfaces the data passes through, we will retrace the entire path from \gls{scikit} to LAPACK in their respective implementations.


\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=465,
]
def _fit_full(self, X, n_components):
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
     # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=sklearn to scipy, 
label=lst:sklearnTOscipy,
firstnumber=489,
]
    self.mean_ = np.mean(X, axis=0)
    X -= self.mean_

    U, S, Vt = linalg.svd(X, full_matrices=False)
\end{lstlisting}



\noindent
The \gls{scikit} version in consideration is \href{\scikitPCAvIxOxI{_pca}}{1.0.1}.
Here we can see that the data is centered in a first step before being handed over to scipy in the last line of listing \ref{lst:sklearnTOscipy}.
Next up, we will look at where the data enters \texttt{scipy} (\href{\scipyvIxVIIxII{decomp_svd}}{version 1.7.2}).



\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=13,
]
def svd(a, full_matrices=True, compute_uv=True, overwrite_a=False,
        		check_finite=True, lapack_driver='gesdd'):
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
    # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=108,
]
    a1 = _asarray_validated(a, check_finite=check_finite)
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
    # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=119,
]
    funcs = (lapack_driver, lapack_driver + '_lwork')
    gesXd, gesXd_lwork = get_lapack_funcs(funcs, (a1,), ilp64='preferred')
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
    # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=scipy to LAPACK, 
label=lst:scipyToLAPACK,
firstnumber=127,
]
    u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,
                full_matrices=full_matrices, overwrite_a=overwrite_a)
\end{lstlisting}



\noindent
To start of the procedure, the matrix is first checked by \texttt{scipy} to make sure that the input does not contain any infinities or \texttt{NaN}s (line 108 in listing \ref{lst:scipyToLAPACK}).
Then, the adequate subroutine call to LAPACK is returned into the current scope based on the keyword \texttt{`lapack\_driver`}.

Finally, the matrix is handed over to the LAPACK routine where it is being solved based on its selected driver.
We will now look at what is hiding behind the intials \texttt{`gesXd`}.
\bigskip

Generally, LAPACK's names for routines are six digits long due to the character limitation imposed by standard Fortran 77.
All drivers and computational routines follow the form of \texttt{XYYZZZ} where \texttt{X} describes the data type, \texttt{YY} the type of matrix and \texttt{ZZZ} the performed computation \cite{anderson1999lapack}.
In \texttt{scipy}'s implementation, the first letter is not considered yet as it will be automatically detected later on in the file \href{\scipyvIxVIIxII{blas}}{\texttt{blas.py}} as following in listing \ref{lst:findBestDataType}:

\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=scipy detect best data type, 
label=lst:findBestDataType,
firstnumber=345,
]
prefix, dtype, prefer_fortran = find_best_blas_type(arrays, dtype)
\end{lstlisting}

After finding the best suited data type in \texttt{scipy}, the characters \texttt{ge} in LAPACK describe a general matrix (i.e. unsymmetric and potentially rectangular).
Ultimately, the characters \texttt{sXd} describe two different routines to solve an \gls{svd}.
\texttt{svd} refers to the so called \emph{simple driver}, the function is refered to as \texttt{xGESVD} in LAPACK.
\texttt{sdd} refers to the \emph{divide and conquer driver} offered by LAPACK, refered to as \texttt{xGESDD}.
\medskip

The \emph{divide and conquer driver} solves the same problem as the simple driver.
Albeit, it is much faster than the simple driver but requires more computational resources.
This routine is chosen by default when calling \texttt{scipy.linalg.svd}, which is exactly what \gls{scikit} does in listing \ref{lst:scipyToLAPACK}.

The full details on the algorithm is found in section 3.4.3 in \citetitle{anderson1999lapack} \cite{anderson1999lapack} and the implementation in Fortran is found in ``Part 2 - Specifications of Routines''.
\bigskip

And indeed, the results are remarkable: \texttt{xGESVD}, has a complexity of $\bigo{n^3}$ \cite{wright2001large}.
This routine is still the default when calling an \gls{svd} in Matlab for example.
\texttt{xGESDD} on the other hand, is able to cut down the complexity significantly and achieves the task in $\bigo{n^2}$.

% \item Overview of the theory behind SVD
% \item scikit implements the svd from LAPACK
% \item scikit uses the LAPACK interface from \cite{2020SciPy} 
% \item LAPACK user guide \cite{anderson1999lapack}
% \item According to \cite{anderson1999lapack}
% $$\bigo{N^2}$$

\clearpage


\subsubsection{Minka's Automatic Choice of Dimensionality for PCA} \label{sec:mle}

One key issue in \acrlong{pca} is how to decide on which \glspl{pc} to keep.
When our goal is to visualise data, the answer might be trivial: 2 or 3, since higher dimensionality is difficult to picture.
But when our goal is efficient computation, the answer is quite more complicated and is tensioned between significantly faster results versus more required computational power.
\bigskip


For this, \citeauthor{minka2000automatic} has proposed an estimation which is alleged to be efficient yet precise \cite{minka2000automatic}.
To achieve this, he developed a Bayesian 


\begin{itemize}
	\item scikit-learn implemented this version \cite{minka2000automatic} to guess the output dimesionality. 
	\item \cite{halko2011finding, brunton2019data} is used to decompose the input matrix.
\end{itemize}

\ \clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{ARPACK / truncated SVD}

\begin{itemize}
	\item \cite{wright2001large} ARPACK approach from scikit
	\item NOT for sparse data, not reviewed here (\href{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html}{scikit})
	\item Optimal rank-r approximation to X is given thanks to the Eckart-Young theorem \cite{eckart1936approximation}
	()
	\item According to \cite{wright2001large, brunton2019data}
	$$\bigo{N^2}$$

\end{itemize}

\ \clearpage
\ \clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Randomized SVD}

\begin{itemize}
	\item \cite{halko2011finding} scikit implementation
	\item \cite{martinsson2011randomized} scikit implementation
	\item \cite{brunton2019data} either capture a pre determined amount of the variance or identify transition points of singular values that give represent important patterns to noise
	\item Increasingly important due to ever growing data set \cite{brunton2019data} (1.1)
	\item According to this \cite{HandsOnMLCh8}
	$$\bigo{d^3}$$
	\item Also mentioned by Brunton: \cite{sarlos2006improved}
\end{itemize}

\ \clearpage
\ \clearpage

