Now that we are familiar with the ideas behind \gls{svd} and aware of the connection between \gls{pca} and \gls{svd}, it is time to see how \gls{pca} is implemented in \gls{scikit}.
We will explore three different possible solvers to decompose large matrices and evaluate their strengths and weaknesses.
\bigskip



In order to select the right solver for the problem, which are all based on \gls{svd}, \gls{scikit}'s \texttt{PCA} class provides the keyword argument \texttt{`solver`} to set one of the following strings: \texttt{`auto`, `full`, `randomized`, `arpack`}.
All of these can be set manually, or if set to \texttt{`auto`}, scikit will choose the one it deems fitting.
\bigskip

The current implementation looks as following:%
\footnote{\href{\scikitPCAvIxOxI}{\texttt{sklearn.decomposition.PCA} (v. 1.0.1)}}






\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=447,
]
if max(X.shape) <= 500 or n_components == "mle":
    self._fit_svd_solver = "full"
elif n_components >= 1 and n_components < 0.8 * min(X.shape):
    self._fit_svd_solver = "randomized"
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
# [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=scikit implementation to select\\an adequate PCA solver, 
label=lst:autoPCA,
firstnumber=452,
]
else:
    self._fit_svd_solver = "full"
\end{lstlisting}


\medskip\noindent
As we can see in listing \ref{lst:autoPCA} above, the choice of the adequate solver is primarily dependent on the parameter \texttt{`n\_components`}.
This keyword allows us to explicitly declare the number of \acrlongpl{pc} to keep.

\texttt{`n\_components`} accepts either a \texttt{float}, which will select the number of components such that the quota of variance desired is reached; an \texttt{int}, which will select as many principal components; or the string \texttt{`mle`}, which estimates a reasonable number of \gls{pc} to keep and will be discussed in greater detail in the upcoming section \ref{sec:mle}.
\bigskip


If the number of \gls{pc} to choose is either not declared or a quota of variance is desired, the solver will always choose \texttt{`full`}.
If a set number of \gls{pc} is requested, the solver will be set on \texttt{`randomized`} as long as the number of \gls{pc} is lower than 80\% of the smallest dimension of the data set.


\ \clearpage


\subsubsection{Full SVD}

First and foremost, we will take a look at the most straightforward approach. 

\begin{itemize}
	\item Overview of the theory behind SVD
	\item scikit implements the svd from LAPACK
	\item scikit uses the LAPACK interface from \cite{2020SciPy} 
	\item LAPACK user guide \cite{anderson1999lapack}
	\item According to \cite{wright2001large}
	$$\bigo{N^3}$$
\end{itemize}

\ \clearpage
\ \clearpage


\subsubsection{Minka's method to approximate intrinsic dimension} \label{sec:mle}

\begin{itemize}
	\item scikit-learn implemented this version \cite{minka2000automatic} to guess the output dimesionality. 
	\item \cite{halko2011finding, brunton2019data} is used to decompose the input matrix.
\end{itemize}

\ \clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{ARPACK / truncated SVD}

\begin{itemize}
	\item \cite{wright2001large} ARPACK approach from scikit
	\item NOT for sparse data, not reviewed here (\href{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html}{scikit})
	\item Optimal rank-r approximation to X is given thanks to the Eckart-Young theorem \cite{eckart1936approximation}
	\item According to \cite{wright2001large, brunton2019data}
	$$\bigo{N^2}$$

\end{itemize}

\ \clearpage
\ \clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Randomized SVD}

\begin{itemize}
	\item \cite{halko2011finding} scikit implementation
	\item \cite{martinsson2011randomized} scikit implementation
	\item \cite{brunton2019data} either capture a pre determined amount of the variance or identify transition points of singular values that give represent important patterns to noise
	\item Increasingly important due to ever growing data set \cite{brunton2019data} (1.1)
	\item According to this \cite{HandsOnMLCh8}
	$$\bigo{d^3}$$
	\item Also mentioned by Brunton: \cite{sarlos2006improved}
\end{itemize}

\ \clearpage
\ \clearpage

