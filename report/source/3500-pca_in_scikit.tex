Now that we are familiar with the ideas behind \gls{svd} and aware of the connection between \gls{pca} and \gls{svd}, it is time to see how \gls{pca} is implemented in \gls{scikit}.
We will explore three different possible solvers to decompose large matrices and evaluate their strengths and weaknesses.
\bigskip



In order to select the right solver for the problem, which are all based on \gls{svd}, \gls{scikit}'s \texttt{PCA} class provides the keyword argument \texttt{`solver`} to set one of the following strings: \texttt{`auto`, `full`, `randomized`, `arpack`}.
All of these can be set manually, or if set to \texttt{`auto`}, scikit will choose the one it deems fitting.
\bigskip

The current implementation looks as following:%
\footnote{\href{\scikitPCAvIxOxI}{\texttt{sklearn.decomposition.PCA} (v. 1.0.1)}}






\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=447,
]
if max(X.shape) <= 500 or n_components == "mle":
    self._fit_svd_solver = "full"
elif n_components >= 1 and n_components < 0.8 * min(X.shape):
    self._fit_svd_solver = "randomized"
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
# [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=scikit implementation to select\\an adequate PCA solver, 
label=lst:autoPCA,
firstnumber=452,
]
else:
    self._fit_svd_solver = "full"
\end{lstlisting}


\medskip\noindent
As we can see in listing \ref{lst:autoPCA} above, the choice of the adequate solver is primarily dependent on the parameter \texttt{`n\_components`}.
This keyword allows us to explicitly declare the number of \acrlongpl{pc} to keep.

\texttt{`n\_components`} accepts either a \texttt{float}, which will select the number of components such that the quota of variance desired is reached; an \texttt{int}, which will select as many principal components; or the string \texttt{`mle`}, which estimates a reasonable number of \gls{pc} to keep and will be discussed in greater detail in the upcoming section \ref{sec:mle}.
\bigskip


If the number of \gls{pc} to choose is either not declared or a quota of variance is desired, the solver will always choose \texttt{`full`}.
If a set number of \gls{pc} is requested, the solver will be set on \texttt{`randomized`} as long as the number of \gls{pc} is lower than 80\% of the smallest dimension of the data set.


\ \clearpage


\subsubsection{Full SVD}

To get a detailed overview of the different solvers, we will start with the default value which is simultaneously the most prominent one.
Following \gls{scikit}'s documentation, we know that \cite{scikit-learn} implements the standard LAPACK solver via \texttt{scipy.linalg.svd}.
\bigskip

LAPACK is an acronym for Linear Algebra Package and is a library of Fortran 77 subroutines which solves most commonly occuring problems in linear algebra \cite{anderson1999lapack}.
It is designed to be maximally efficient on the whole spectrum of modern computers, with a particular focus on high-performance computers.

In order to understand the parameters and interfaces the data passes through, we will retrace the entire path from \gls{scikit} to LAPACK in their respective implementations.


\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=465,
]
def _fit_full(self, X, n_components):
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
     # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=sklearn to scipy, 
label=lst:sklearnTOscipy,
firstnumber=489,
]
    self.mean_ = np.mean(X, axis=0)
    X -= self.mean_

    U, S, Vt = linalg.svd(X, full_matrices=False)
\end{lstlisting}



\noindent
The \gls{scikit} version in consideration is \href{\scikitPCAvIxOxI}{1.0.1}.
Here we can see that the data is centered in a first step before being handed over to scipy in the last line of listing \ref{lst:sklearnTOscipy}.
Next up, we will look at where the data enters \texttt{scipy} (\href{\scipyvIxVIIxII}{version 1.7.2}).



\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=13,
]
def svd(a, full_matrices=True, compute_uv=True, overwrite_a=False,
        		check_finite=True, lapack_driver='gesdd'):
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
    # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=108,
]
    a1 = _asarray_validated(a, check_finite=check_finite)
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
    # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=119,
]
    funcs = (lapack_driver, lapack_driver + '_lwork')
    gesXd, gesXd_lwork = get_lapack_funcs(funcs, (a1,), ilp64='preferred')
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
    # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=scipy to LAPACK, 
label=lst:scipyToLAPACK,
firstnumber=127,
]
    u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,
                full_matrices=full_matrices, overwrite_a=overwrite_a)
\end{lstlisting}



\noindent
To start of the procedure, the matrix is first checked by \texttt{scipy} to make sure that the input does not contain any infinities or \texttt{NaN}s (line 108 in listing \ref{lst:scipyToLAPACK}).
Then, the adequate subroutine call to LAPACK is returned into the current scope based on the keyword \texttt{`lapack\_driver`}.

Finally, the matrix is handed over to the LAPACK routine where it is being solved based on its selected driver.
We will now look at what is hiding behind the intials \texttt{`gesXd`}.

\clearpage

\begin{itemize}
	\item Overview of the theory behind SVD
	\item scikit implements the svd from LAPACK
	\item scikit uses the LAPACK interface from \cite{2020SciPy} 
	\item LAPACK user guide \cite{anderson1999lapack}
	\item According to \cite{wright2001large}
	$$\bigo{N^3}$$
\end{itemize}

\clearpage


\subsubsection{Minka's method to approximate intrinsic dimension} \label{sec:mle}

\begin{itemize}
	\item scikit-learn implemented this version \cite{minka2000automatic} to guess the output dimesionality. 
	\item \cite{halko2011finding, brunton2019data} is used to decompose the input matrix.
\end{itemize}

\ \clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{ARPACK / truncated SVD}

\begin{itemize}
	\item \cite{wright2001large} ARPACK approach from scikit
	\item NOT for sparse data, not reviewed here (\href{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html}{scikit})
	\item Optimal rank-r approximation to X is given thanks to the Eckart-Young theorem \cite{eckart1936approximation}
	\item According to \cite{wright2001large, brunton2019data}
	$$\bigo{N^2}$$

\end{itemize}

\ \clearpage
\ \clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Randomized SVD}

\begin{itemize}
	\item \cite{halko2011finding} scikit implementation
	\item \cite{martinsson2011randomized} scikit implementation
	\item \cite{brunton2019data} either capture a pre determined amount of the variance or identify transition points of singular values that give represent important patterns to noise
	\item Increasingly important due to ever growing data set \cite{brunton2019data} (1.1)
	\item According to this \cite{HandsOnMLCh8}
	$$\bigo{d^3}$$
	\item Also mentioned by Brunton: \cite{sarlos2006improved}
\end{itemize}

\ \clearpage
\ \clearpage

