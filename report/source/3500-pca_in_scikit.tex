Now that we are familiar with the ideas behind the \gls{svd} and are aware of the connection between \gls{pca} and the \gls{svd}, it is time to see how \gls{pca} is implemented in \gls{scikit}.
We will explore three different possible solvers to decompose large matrices and evaluate their strengths and weaknesses.
\bigskip



In order to select the right solver for the problem, which are all based on the \gls{svd}, \gls{scikit}'s \texttt{PCA} class provides the keyword argument \texttt{`solver`} to be set to one of the following strings: \texttt{`auto`, `full`, `randomized`} or \texttt{`arpack`}.
The latter three can be set manually, or if set to \texttt{`auto`}, scikit will choose the one it deems fitting.
\bigskip

The current implementation to choose an adequate solver looks as following:%
\footnote{\href{\scikitPCAvIxOxI{_pca}}{\texttt{sklearn.decomposition.PCA} (v. 1.0.1)}}






\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=447,
]
if max(X.shape) <= 500 or n_components == "mle":
    self._fit_svd_solver = "full"
elif n_components >= 1 and n_components < 0.8 * min(X.shape):
    self._fit_svd_solver = "randomized"
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
# [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=scikit implementation to select\\an adequate PCA solver, 
label=lst:autoPCA,
firstnumber=452,
]
else:
    self._fit_svd_solver = "full"
\end{lstlisting}


\medskip\noindent
As we can see in listing \ref{lst:autoPCA} above, the choice of the adequate solver is primarily dependent on the parameter \texttt{`n\_components`}.
This keyword allows us to explicitly declare the number of \acrlongpl{pc} to keep.

\texttt{`n\_components`} accepts either a \texttt{float}, which will select the number of components such that a given quota of a desired variance is reached; an \texttt{int}, which will select as many principal components; or the string \texttt{`mle`}, which estimates a reasonable number of \gls{pc} to keep and will be discussed in greater detail in the upcoming section \ref{sec:mle}.
\bigskip


If the number of \gls{pc} to choose is either not declared or a quota of variance is desired, the solver will always choose \texttt{`full`}.
If a set number of \gls{pc} is requested, the solver will be set to \texttt{`randomized`} as long as the number of \gls{pc} is lower than 80\% of the smallest dimension of the data set.


\ \clearpage


\subsubsection{Full SVD} \label{sec:fullSVD}

To get a detailed overview of the different solvers, we will start with the default value which is simultaneously the most prominent one.
Following \gls{scikit}'s documentation, we know that \gls{scikit} implements the standard \acrshort{lapack} solver via \texttt{scipy.linalg.svd}.
\bigskip

\acrshort{lapack} is an acronym for Linear Algebra Package and is a library of Fortran 77 subroutines which solves most commonly occurring problems in linear algebra \cite{anderson1999lapack}.
It is designed to be maximally efficient on the whole spectrum of modern computers, with a particular focus on high-performance computers.

To understand the parameters and interfaces the data passes through, we will retrace the entire path from \gls{scikit} to \acrshort{lapack} in their respective implementations.


\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=465,
]
def _fit_full(self, X, n_components):
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
     # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=sklearn to \gls{scipy}, 
label=lst:sklearnTOscipy,
firstnumber=489,
]
    self.mean_ = np.mean(X, axis=0)
    X -= self.mean_

    U, S, Vt = linalg.svd(X, full_matrices=False)
\end{lstlisting}



\noindent
The \gls{scikit} version in consideration is \href{\scikitPCAvIxOxI{_pca}}{1.0.1}.
Here we can see that the data is centred in a first step before being handed over to \gls{scipy} in the last line of listing \ref{lst:sklearnTOscipy}.
Next up, we will look at where the data enters \gls{scipy} (\href{\scipyvIxVIIxII{decomp_svd}}{version 1.7.2}).



\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=13,
]
def svd(a, full_matrices=True, compute_uv=True, overwrite_a=False,
        		check_finite=True, lapack_driver='gesdd'):
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
    # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=108,
]
    a1 = _asarray_validated(a, check_finite=check_finite)
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
    # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python,
firstnumber=119,
]
    funcs = (lapack_driver, lapack_driver + '_lwork')
    gesXd, gesXd_lwork = get_lapack_funcs(funcs, (a1,), ilp64='preferred')
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[language=python, numbers=none]
    # [...]
\end{lstlisting}
%
%
%
\spacingConcatLists
\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=\gls{scipy} to \acrshort{lapack}, 
label=lst:scipyToLAPACK,
firstnumber=127,
]
    u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,
                full_matrices=full_matrices, overwrite_a=overwrite_a)
\end{lstlisting}



\noindent
To start of the procedure, the matrix is first checked by \gls{scipy} to make sure that the input does not contain any infinities or \texttt{NaN}s (line 108 in listing \ref{lst:scipyToLAPACK}).
Then, the adequate subroutine call to \acrshort{lapack} is returned into the current scope based on the keyword \texttt{`lapack\_driver`}.

Finally, the matrix is handed over to the \acrshort{lapack} routine where it is being solved based on its selected driver.
We will now look at what is hiding behind the initials \texttt{`gesvd`} and \texttt{`gessd`}.
\bigskip

Generally, \acrshort{lapack} names for routines are six digits long due to the character limitation imposed by standard Fortran 77.
All drivers and computational routines follow the form of \texttt{XYYZZZ} where \texttt{X} describes the data type, \texttt{YY} the type of matrix and \texttt{ZZZ} the performed computation \cite{anderson1999lapack}.
In the implementation of \gls{scipy}, the first letter is not considered yet as it will be detected automatically later on in the module \href{\scipyvIxVIIxII{blas}}{\texttt{blas.py}} as following in listing \ref{lst:findBestDataType}:

\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=\gls{scipy} detect best data type, 
label=lst:findBestDataType,
firstnumber=345,
]
prefix, dtype, prefer_fortran = find_best_blas_type(arrays, dtype)
\end{lstlisting}

After finding the best suited data type in \gls{scipy}, the characters \texttt{ge} in \acrshort{lapack} describe a general matrix (i.e., unsymmetric and potentially rectangular).
Ultimately, the characters \texttt{svd} and \texttt{sdd} describe two different routines to solve a \gls{svd}.
\texttt{svd} refers to the so-called \emph{simple driver}, the function is referred to as \texttt{xGESVD} in \acrshort{lapack}.
\texttt{sdd} refers to the \emph{divide and conquer driver} offered by \acrshort{lapack}, referred to as \texttt{xGESDD}.
\medskip

The \emph{divide and conquer driver} solve the same problem as the simple driver.
It is much faster than the simple driver but requires more computational resources.
This routine is chosen by default when calling \texttt{scipy.linalg.svd}, which is exactly what \gls{scikit} does in listing \ref{lst:scipyToLAPACK}.

The full detail on the algorithm is found in section 3.4.3 in \citetitle{anderson1999lapack} \cite{anderson1999lapack} and the implementation in Fortran 77 is found in ``Part 2 - Specifications of Routines''.
\bigskip

And indeed, the results are remarkable: \texttt{xGESVD}, has a complexity of $\bigo{n^3}$ \cite{wright2001large}.
This routine is still the default when calling an \gls{svd} in Matlab for example.
\texttt{xGESDD} on the other hand, chosen by default by scikit and \gls{scipy}, is able to cut down the complexity significantly and achieves the task in $\bigo{n^2}$\cite{anderson1999lapack}.

% \item Overview of the theory behind SVD
% \item scikit implements the svd from \acrshort{lapack}
% \item scikit uses the \acrshort{lapack} interface from \cite{2020SciPy} 
% \item \acrshort{lapack} user guide \cite{anderson1999lapack}
% \item According to \cite{anderson1999lapack}
% $$\bigo{N^2}$$

\clearpage


\subsubsection{Minka's Automatic Choice of Dimensionality for PCA} \label{sec:mle}

One key issue in \acrlong{pca} is how to decide which \glspl{pc} to keep.
When our goal is to visualise data, the answer is trivial: 2 or 3 since higher dimensionality is difficult to picture.
But when our goal is efficient computation, the answer is quite more complicated and is tensioned between significantly faster results versus higher requirements in computational power.
\bigskip


For this, \citeauthor{minka2000automatic} has proposed an estimation which is alleged to be efficient yet precise \cite{minka2000automatic}.
To compute this approximation, we are starting from a Bayesian model assumption which states that the probability of the data $D$, given a model $M$, is found by finding the \gls{lineintegral} over the parameter $\theta$:

\begin{equation}
	\label{formula:minkaMLE}
	p(D | M) = \int_\theta \ p(D | \theta)\ p(\theta | M)\ d\theta
\end{equation}

\medskip\noindent
This integral can then be approximated efficiently using a Laplace approximation.
\bigskip


His method did not only turn out to be significantly faster than its contestants.
The simulations he performed turned out to be convincingly more accurate and guaranteed to pick the correct dimensionality for an accurate representation of the data.

\bigskip

To round this up, \citeauthor{minka2000automatic} tested the performance in his various samples.
He used data-rich, sparse as well as data sets with a varying signal-to-noise ratios.

The results are exceptional, while not always scoring best amongst all different algorithms for model selection, his approach is able to consistently perform well accross various disciplines.
This is opposing to other algorithms, which perform well on a specific shape of data, but utterly fail in different categories.



\bigskip
The function \texttt{\_assess\_dimensions} in lines 30 -- 101 in the module \href{\scikitPCAvIxOxI{_pca}}{\texttt{\_pca} in version 1.0.1} in \gls{scikit} implements \citeauthor{minka2000automatic}'s proposed estimation.


% \item scikit-learn implemented this version \cite{minka2000automatic} to guess the output dimesionality. 
% \item \cite{halko2011finding, brunton2019data} is used to decompose the input matrix.

\ \clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Truncated SVD}

Next in line, we will see why \gls{scikit} refers to \texttt{scipy.sparse.linalg.svds} when choosing the solver \texttt{`arpack`}. 
\acrshort{arpack} stands for Arnoldi Package and is a different collection of Fortran 77 routines and partially dependent on \acrshort{lapack}.
It is designed exclusively to solve large-scale eigenvalue problems \cite{lehoucq1998arpack}.
\medskip


We will now again trace the flow of data from \gls{scikit} through \gls{scipy} into the package where it is being processed.
This starts again the the \href{\scikitPCAvIxOxI{_pca}}{\texttt{\_pca}} module:

\begin{lstlisting}[
inputencoding={utf8}, 
extendedchars=false, 
language=python, 
caption=scikit to \gls{scipy}, 
label=lst:scikitTOscipy,
firstnumber=569,
]
if svd_solver == "arpack":
    v0 = _init_arpack_v0(min(X.shape), random_state)
    U, S, Vt = svds(X, k=n_components, tol=self.tol, v0=v0)
\end{lstlisting}

\noindent
The call to \texttt{\_init\_arpack\_v0} will sample random vectors from the uniform distribution of the data.
The parameter \texttt{random\_state} will make sure that the random vectors are merely pseudorandom and provide a deterministic result.
Now, we will look at how the data is traversing \gls{scipy} after being called in line 571 of listing \ref{lst:scikitTOscipy}.
\bigskip

\renewcommand{\tikzscale}{0.65}
\begin{figure}[h]
	\centering
	\input{source/3531-flow_scikit_to_arpack.tex}
	\captionsetup{justification=centering}
	\vspace*{5mm}
	\caption{From scikit to \acrshort{arpack}.}
	\label{fig:scikitToARPACK}
\end{figure}
\footnotetext{\texttt{\_SymmetricArpackParams}}
\bigskip


Now, we will describe what each node in figure \ref{fig:scikitToARPACK} represents:
\begin{itemize}
	\item Starting in \texttt{scikit}, we arrive in the second node \texttt{svds}, which is the function being called in the \gls{scipy} collection \texttt{scipy.sparse.linalg.svds}.
	\item \texttt{svds} then analyses the shape of the matrix to determine the most efficient sequence of multiplication (either $A^TA$ or $AA^T$) before passing it to the next function \texttt{eigsh}.
	\item \texttt{eigsh} then verifies the content of the matrix and chooses the adequate \acrshort{arpack} driver:
	\begin{itemize}
		\item If the matrix is identified a Hermitian (complex) matrix, then the complex driver is used. As we are only focusing on real (symmetric) matrices, this step will not be considered any further.
		\item If the matrix is identified to be rectangular instead of squared, the function is handed to a \acrshort{lapack} driver instead.
		\item If the matrix is confirmed to be symmetric as expected, the matrix is processed by the function \texttt{\_SymmetricArpackParams} (\texttt{\_SAP}) and then handed over to the \texttt{SAUPD} driver.
	\end{itemize}
\end{itemize}

The \texttt{SAUPD} driver in \acrshort{arpack} then solves the eigenproblem as a variant of the Lanczos method.
The Lanczos method is designed to compute approximations to a few eigenvalues and eigenvectors of a symmetric matrix \cite{lehoucq1998arpack}.

\texttt{SAUPD} can then be finetuned with various parameters such as \texttt{WHICH} to compute the smallest or largest eigenvalues first, \texttt{NEV} to set the numbers of eigenvalues to be computed; or \texttt{TOL} to set the tolerance of the precision for floating point numbers.
\bigskip

To sum up the \acrshort{arpack} method, it is a procedure to approximate the decomposition of a matrix.
Thanks to the approximation, the runtime does not exceed $\bigo{N^2}$ \cite{brunton2019data, wright2001large}.

Knowing the \texttt{xGESDD} method from \acrshort{lapack} from the previous section \ref{sec:fullSVD}, the \acrshort{arpack} approaches primary benefit is its lenient resource utilisation \cite{anderson1999lapack}.
This makes it a lot more suited for large scale problems while simultaneously explaining why \gls{scikit} does not implement it by default using the \texttt{auto} solver.

\vspace*{20mm}

% \item \cite{wright2001large} \acrshort{arpack} approach from scikit
% \item NOT for sparse data, not reviewed here (\href{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html}{scikit})
% \item Optimal rank-r approximation to X is given thanks to the Eckart-Young theorem \cite{eckart1936approximation}
% \item According to \cite{brunton2019data, wright2001large}
% $$\bigo{N^2}$$
% \item but the $\bigo{N^2}$ from the \texttt{SEGSDD} requires more resources \cite{anderson1999lapack}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Randomized SVD}


As a last approach to establishing the \acrlong{svd} of a matrix, we will look at opportunities that are made possible by the theory of random sampling \cite{brunton2019data}. 
The primary motivation for random methods is the increasingly vast measurements available across many different disciplines.

Fortunately, the intrinsic rank of the data does not increase significantly if at all.
Thanks to this awareness, randomised numerical methods provide an accurate matrix decomposition at a fraction of the computational cost of the previously introduced models \cite{brunton2019data}.
Over the past two decades, there have been numerous different propositions to randomised algorithms.
\medskip

One that has emerged in a wide range of disciplines has been the method proposed by \mycite{halko2011finding}.
Simultaneously, this is the method implemented in \gls{scikit}.
\bigskip


We will now go through the idea of the random \gls{svd} step-by-step.
To begin with, we consider a matrix $X \in \mathbb{R}^{n \times m}$.
Our goal now is to multiply this by a random matrix $P \in \mathbb{R}^{m \times r}$.\footnote{%
$P$ often consists of values lieing within a standard normal distribution due to its favourable mathematical properties \cite{brunton2019data}. %
However, other --- less computationally expensive --- random matrices are also frequently used.%
}
When looking at the number of columns in $P$, $r$ represents the number of \acrlongpl{pc} we want to compute.
After this, we will obtain a new matrix $Z \in \mathbb{R}^{n \times r}$:

\vspace{-4mm}
\begin{equation}
	\label{formula:rSVDstepONE}
	Z = X P
\end{equation}

Now it is time to compute the low-rank \gls{qr} of $Z$.
$Q \in \mathbb{R}^{n \times r}$, $R \in \mathbb{R}^{r \times r}$. 
This will result in $Z$ being an \gls{onb} for $X$:

\vspace{-4mm}
\begin{equation}
	\label{formula:rSVDstepTWO}
	Z = Q R
\end{equation}

Thanks to the low-rank basis $Q$, we can now project $X$ into a reduced space which will result in $Y \in \mathbb{R}^{r \times m}$:

\vspace{-4mm}
\begin{equation}
	\label{formula:rSVDstepTHREE}
	Y = Q^T X
\end{equation}

It now follows that $X \approx QY$ \cite{brunton2019data} and it is now possible to compute the \gls{svd} based on Y:

\vspace{-4mm}
\begin{equation}
	\label{formula:rSVDstepFOUR}
	Y = U_Y \ \Sigma \ V^T
\end{equation}

Because Q is orthonormal and approximates the columns of $X$, the matrices singular values $\Sigma$ and the right-singular vectors $V$ are the same for $Y$ and $X$ \cite{brunton2019data}.

Finally, thanks to the aforementioned properties, it is now possible to reconstruct the high-dimensional left singular vectors $U$ using $U_Y$ and $Q$:

\vspace{-4mm}
\begin{equation}
	\label{formula:rSVDstepFIVE}
	U = Q U_Y
\end{equation}
\smallskip


Now that we are familiar how to approximate matrix decompositions with randomness, let us consider two methods on how to control and improve the prediction results.
We will now introduce the concepts of oversampling as well as power iterations.

\paragraph{Oversampling}

Oversampling is based on the idea that most matrices (such as $X$) do not have a predictable low-rank structure.
We are unlikely to guess it right.
Therefore, when computing the matrix $P \in \mathbb{R}^{m \times r}$, we will simply add a certain number of additional columns.
This number can be in a range as low as 5 to 10 and significantly improve results \cite{martinsson2016randomized}.
The resulting matrix $P$ then could resemble $P \in \mathbb{R}^{m \times r+10}$.
\gls{scikit} implements the parameter \texttt{n\_oversamples} for this purpose and sets the default value to 10.



\paragraph{Power Iterations}

The idea is to pre-process a data matrix $X$ and to take it to a parametrised power $q$.
This results in:

\vspace{-4mm}
\begin{equation}
	\label{formula:powerIterations}
	X^{(q)} = (XX^T)^q X
\end{equation}

Using power iterations, we lengthen the distance between the significant ranks and the redundant information.
Power iterations are an effective way to improve predictive performance \cite{halko2011finding}.
However, they come with the drawback of being particularly expensive to compute.
\gls{scikit} implements the parameter \texttt{n\_iter} which defaults to 4\footnote{%
unless the number of \glspl{pc} to compute is small, then it will default to 7%
}.
\bigskip
\bigskip


The randomised \gls{svd} allows for a computational complexity of $\bigo{d^3}$ \cite{HandsOnMLCh8}.
This approach to matrix decomposition paves the way for petascale data analysis.
The gain in efficiency is not solely owed to the nature of computing based on random lower dimensional matrices but also additionally allows efficient processing in multi-processor architectures \cite{halko2011finding}.


% \item \cite{halko2011finding} scikit implementation
% \item \cite{martinsson2011randomized} scikit implementation
% \item \cite{brunton2019data} either capture a pre determined amount of the variance or identify transition points of singular values that give represent important patterns to noise
% \item Increasingly important due to ever growing data set \cite{brunton2019data} (1.1)
% \item According to this \cite{HandsOnMLCh8}
% $$\bigo{d^3}$$
% \item Also mentioned by Brunton: \cite{sarlos2006improved}

\clearpage
