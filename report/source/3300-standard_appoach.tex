\subsubsection{Projection}



Projection can either be considered as an approach in its own or as a simple interpretation of the geometrical idea behing \gls{pca} in general.
The idea is to project the data points onto a \gls{hyperplane} which summarises the data with as little information loss as possible.
This newly defined hyperplane can then be considered a principal component of the data set.


\renewcommand{\tikzscale}{0.4}
\begin{figure}[h]
	\centering
	\input{source/3301-projection_example.tex}
	\captionsetup{justification=centering}
	\vspace*{4mm}
	\caption{Toy example of a projection}
    \label{fig:projectionExample}
\end{figure}


Figure \ref{fig:projectionExample} illustrates this in a toy example.
As we can observe, when we pick the right \gls{hyperplane}, such as the x axis shown above, we lose far fewer information than if we would have picked the y axis.\bigskip


As one might be inclined to think the idea strongly ressembles to a regression problem. 
\citeauthor{tarpey1999self} elaborates this idea thoroughly in his paper \citetitle{tarpey1999self} \cite{tarpey1999self}.
\citeauthor{tarpey1999self} ends his work by proposing a test for the determined ``self-consistency'' of a principal component.


% \item \cite{tarpey1999self} has good approaches to get behind the idea of projection
% \item minimizing the distance of orthogonal projections hints to the first principal component
% \item Geometric interpretation of the problem 
% \item Is a regression problem \cite{brunton2019data} (1.4)




\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Correlation and Covariance} \label{section:correlationANDcovariance}

As it is often true when dealing with large data set, the best suited choice for a problem depend on the given circumstances themselves and does not have a generalised answer. 

Both measures of the variation between two variables, the covariance and the corrleation, are essential to the various approaches to \gls{pca}.
Therefore, we will begin by looking at the definitions of the covariance and correlation between two variables and highlight their similarities \cite{downey2011think}:


\vspace{-28mm}
{\setstretch{3}
\begin{align}
	\label{formula:covariance}
	Cov(X,Y) &= \frac{1}{n} \cdot \displaystyle\sum_{i=0}^{n} (x_i - \overline{x})\cdot(y_i - \overline{y})\bigskip
	\\
	\label{formula:correlation}
	Corr(X,Y) &= \frac{1}{n} \cdot \displaystyle\sum_{i=0}^{n} \frac{(x_i - \overline{x})}{\sigma_X}\cdot\frac{(y_i - \overline{y})}{\sigma_Y}
\end{align}
}

\vspace{-10mm}
With $X$ and $Y$ being two series of variables, $x_i$ and $y_i$ their members, $n$ the size of the series, $\overline{x}$ and $\overline{y}$ the mean and $\sigma$ the respective standard deviation of the series.

As we can observe, the definitions are tied to each other. By factoring out $\sigma_X$ and $\sigma_Y$, the correlation can be rewritten as:

\vspace{-6mm}
\begin{align}
	\label{formula:covarianceFTcorrelation}
	Corr(X,Y) &= \frac{Cov(X,Y)}{\sigma_X \cdot \sigma_Y}
\end{align}

Therefore we can conclude that the correlation will always lie in the range of $[-1;1]$ whereas the covariance shows the raw numbers that these variables do represent.\bigskip


Generally, the covariance gives us a way better feel for the values and the relationships they represent and are therefore easy to interpret at first sight.
The correlation matrix however, being of relative nature, requires the proportions of the variables to be known before being able to understand the scope that they represent.
Understanding the proportions is also a concept that has to be remembered by the person who interprets such data in order to not make false assumptions.

However, when the variance between variables is enormous in comparison to other variables in a data set, the covariance can render the assertions from the first components useless as the series with the enormous variance will dominate the calculations.

A detailed example is given in section 3.3 of \citeauthor{brunton2019data}'s book \citetitle{brunton2019data} \cite{brunton2019data}.

% \item Correlation Matrix is the normalized version of the Covariance Matrix (sort of)
% \item Mentioned by \cite{jolliffe2016principal} in Section 2 (a)
% \item Good contrast between the two by \cite{Jolliffe2002book} in section 3.3

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Lagrange Multipliers}

Now that we have established how to compute the covariance and the correlation between two variables, we will defog how to this comes together with our large data sets.
To simplify the scope, we will solely consider the covariance at this point.

To begin with, we have to calculate the covariance between each and every variable inside a data set.
This will result in a covariance matrix.
% PythonMachineLearningCh1 is in fact chapter 10
The covariance matrix, which is a square matrix \cite{PythonMachineLearningCh1}, can then be

\begin{itemize}
	\item Mentioned by \cite{jolliffe2016principal} in Section 2 (a)
	\item more details in introduction chapter in \cite{Jolliffe2002book}
\end{itemize}

\begin{align}
	S \cdot a - \lambda \cdot a &= 0 \\
	\Leftrightarrow S \cdot a &= \lambda \cdot a
\end{align}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Eigendecomposition}

\begin{itemize}
	\item Looks like a good wrapup: \cite{abdi2007eigen}
\end{itemize}

\clearpage

