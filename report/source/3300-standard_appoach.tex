Following the idea from Hotelling \cite{deisenroth2020mathematics, hotelling1933analysis}.

\subsubsection{Projection}



Projection can either be considered as an approach in its own or as a simple interpretation of the geometrical idea behing \gls{pca} in general.
The idea is to project the data points onto a \gls{hyperplane} which summarises the data with as little information loss as possible.
This newly defined hyperplane can then be considered a principal component of the data set.


\renewcommand{\tikzscale}{0.4}
\begin{figure}[h]
	\centering
	\input{source/3301-projection_example.tex}
	\captionsetup{justification=centering}
	\vspace*{4mm}
	\caption{Toy example of a projection}
    \label{fig:projectionExample}
\end{figure}


Figure \ref{fig:projectionExample} illustrates this in a toy example.
As we can observe, when we pick the right \gls{hyperplane}, such as the x axis shown above, we lose far fewer information than if we would have picked the y axis.\bigskip


As one might be inclined to think the idea strongly ressembles to a regression problem. 
\citeauthor{tarpey1999self} elaborates this idea thoroughly in his paper \citetitle{tarpey1999self} \cite{tarpey1999self}.
\citeauthor{tarpey1999self} ends his work by proposing a test for the determined ``self-consistency'' of a principal component.


% \item \cite{tarpey1999self} has good approaches to get behind the idea of projection
% \item minimizing the distance of orthogonal projections hints to the first principal component
% \item Geometric interpretation of the problem 
% \item Is a regression problem \cite{brunton2019data} (1.4)




\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Correlation and Covariance} \label{section:correlationANDcovariance}

As it is often true when dealing with large data set, the best suited choice for a problem depend on the given circumstances themselves and does not have a generalised answer. 

Both measures of the variation between two variables, the covariance and the corrleation, are essential to the various approaches to \gls{pca}.
Therefore, we will begin by looking at the definitions of the covariance and correlation between two variables and highlight their similarities \cite{downey2011think}:


\vspace{-28mm}
{\setstretch{3}
\begin{align}
	\label{formula:covariance}
	Cov(X,Y) &= \frac{1}{n} \cdot \displaystyle\sum_{i=0}^{n} (x_i - \overline{x})(y_i - \overline{y})\bigskip
	\\
	\label{formula:correlation}
	Corr(X,Y) &= \frac{1}{n} \cdot \displaystyle\sum_{i=0}^{n} \frac{(x_i - \overline{x})}{\sigma_X}\cdot\frac{(y_i - \overline{y})}{\sigma_Y}
\end{align}
}

\vspace{-10mm}
With $X$ and $Y$ being two series of variables, $x_i$ and $y_i$ their members, $n$ the size of the series, $\overline{x}$ and $\overline{y}$ the mean and $\sigma$ the respective standard deviation of the series.

As we can observe, the definitions are tied to each other. By factoring out $\sigma_X$ and $\sigma_Y$, the correlation can be rewritten as:

\vspace{-6mm}
\begin{align}
	\label{formula:covarianceFTcorrelation}
	Corr(X,Y) &= \frac{Cov(X,Y)}{\sigma_X \cdot \sigma_Y}
\end{align}

Therefore we can conclude that the correlation will always lie in the range of $[-1;1]$ whereas the covariance shows the raw numbers that these variables do represent.\bigskip


Generally, the covariance gives us a way better feel for the values and the relationships they represent and are therefore easy to interpret at first sight.
The correlation matrix however, being of relative nature, requires the proportions of the variables to be known before being able to understand the scope that they represent.
Understanding the proportions is also a concept that has to be remembered by the person who interprets such data in order to not make false assumptions.

However, when the variance between variables is enormous in comparison to other variables in a data set, the covariance can render the assertions from the first components useless as the series with the enormous variance will dominate the calculations.

A detailed example is given in section 3.3 of \citeauthor{brunton2019data}'s book \citetitle{brunton2019data} \cite{brunton2019data}.

% \item Correlation Matrix is the normalized version of the Covariance Matrix (sort of)
% \item Mentioned by \cite{jolliffe2016principal} in Section 2 (a)
% \item Good contrast between the two by \cite{Jolliffe2002book} in section 3.3

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Lagrange Multipliers}

Now that we have established how to compute the covariance and the correlation between two variables, we will defog how to this comes together with our large data sets.
For now, we will solely consider the covariance.

To begin with, we have to calculate the covariance between each and every variable inside a data set.
This will result in a covariance matrix.
% PythonMachineLearningCh1 is in fact chapter 10
The covariance matrix, which is a square matrix \cite{PythonMachineLearningCh1}, can then be transformed by a vector $\overrightarrow{v}$, expressed by $Var(X\overrightarrow{v})$\footnote{%
	The variance is defined as $Var(X) = \frac{1}{n} \cdot \displaystyle\sum_{i=1}^{n} (x_i - \overline{x})^2$ \cite{deisenroth2020mathematics}
	%
}.
To maximise the variance, we can then solve a constrained opimisation problem.
This will then diminish into an eigenvalue equation.
\bigskip


We will now go through the steps, one-by-one, in order to comprehend how to find an algebraic solution to this problem:


\vspace{-25mm}
{\setstretch{2.75}
\begin{align}
	\label{formula:maxVarI}
	Var(X\overrightarrow{v}) &= \frac{1}{n} \cdot \displaystyle\sum_{i=1}^{n} (\overrightarrow{v}(x_i-\overline{x}))^2
	\\
	\label{formula:maxVarII}
	&= \frac{1}{n} \cdot \displaystyle\sum_{i=1}^{n} (\overrightarrow{v}(x_i-\overline{x})) (\overrightarrow{v}(x_i-\overline{x}))
	\\
	\label{formula:maxVarIII}
	&= \frac{1}{n} \cdot \displaystyle\sum_{i=1}^{n} (\overrightarrow{v}(x_i-\overline{x})) ((x_i-\overline{x})^T \ \overrightarrow{v}^T)
	\\
	\label{formula:maxVarIV}
	&= \frac{1}{n} \cdot \displaystyle\sum_{i=1}^{n} (\overrightarrow{v}(x_i-\overline{x})) ((y_i-\overline{y}) \ \overrightarrow{v}^T)
	\\
	\label{formula:maxVarV}
	&= \overrightarrow{v} \Bigg(\frac{1}{n} \cdot \displaystyle\sum_{i=1}^{n} ((x_i-\overline{x})) ((y_i-\overline{y}))\Bigg) \overrightarrow{v}^T
	\\
	\label{formula:maxVarVI}
	&= \overrightarrow{v} \enskip Cov(X,Y) \enskip \overrightarrow{v}^T
\end{align}
}

\vspace{-16mm}

\begin{itemize}
	\item In step \reff{maxVarIII} we used the fact that the dot product of two vectors is symmetric with respect to its arguments so that $\overrightarrow{v}^T x = x^T\overrightarrow{v}$.
	\item Step \reff{maxVarIV} is due to the transpose of a row-wise covariance matrix being equal to the column-wise covariance matrix.
	\item Finally, before proceeding any further, we will define $S := Cov(X,Y)$.
\end{itemize}


\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now we are able to define an optimisation problem from $\overrightarrow{v} S \overrightarrow{v}^T$.
When trying to compute the problem, we notice that arbitrarily increasing the magnitude of vector $\overrightarrow{v}$ would result in proportionally increasing the results ad infinitum.
Therefore, we need to restrict all solutions to a set size such as $\norm{v} = 1$ (note that $\norm{a}$ represents the Euclidean norm of $a$).\todo{check the order of the transpose}

Now dealing with a constained optimisation problem, we can obtain the Lagrangian $\mathcal{L}(\overrightarrow{v}, \lambda)$ to solve the problem $\max_{\overrightarrow{v}} (\overrightarrow{v} S \overrightarrow{v}^T)$ \cite{deisenroth2020mathematics}:

\begin{equation}
	\label{formula:maxOptimisationI}
	\mathcal{L}(\overrightarrow{v}, \lambda) = \overrightarrow{v}^T S \overrightarrow{v} + \lambda_1 (1 - \overrightarrow{v}^T\overrightarrow{v})
\end{equation}
\begin{equation}
	\label{formula:maxOptimisationII}
	\frac{\partial \mathcal{L}}{\partial \overrightarrow{v}} = 2 \overrightarrow{v}^T S - 2 \lambda_1 \overrightarrow{v}^T,
	\quad \quad
	\frac{\partial \mathcal{L}}{\partial \lambda_1} = 1 - \overrightarrow{v}^T \overrightarrow{v}
\end{equation}

\vspace{2mm}

\begin{itemize}
	\item Step \reff{maxOptimisationI} is the definition of the Lagrangian problem, given that $\overrightarrow{v}^T\overrightarrow{v}=1$, the addition of the right handside does not affect the result.
	\item Let us now take a look at step \reff{maxOptimisationII}, which looks a lot less intuitive. 
	What happens here is the partial derivates of $\mathcal{L}$ with respect to $\overrightarrow{v}$ and $\lambda_1$.
	By setting these partial derivates to zero, this step allows us to conclude this journey:
\end{itemize}

\vspace{-16mm}
{\setstretch{1.5}
\begin{align}
	\label{formula:maxOptimisationIII}
	2 \overrightarrow{v}^T S - 2 \lambda_1 \overrightarrow{v}^T = 0 &= 1 - \overrightarrow{v}^T \overrightarrow{v}
	\\
	\label{formula:maxOptimisationIV}
	2 \overrightarrow{v}^T S - 2 \lambda_1 \overrightarrow{v}^T &= 0
	\\
	\label{formula:maxOptimisationV}
	2 \overrightarrow{v}^T S &= 2 \lambda_1 \overrightarrow{v}^T
	\\
	\label{formula:maxOptimisationVI}
	\overrightarrow{v}^T S &= \lambda_1 \overrightarrow{v}^T
	\\
	\label{formula:maxOptimisationVII}
	S \overrightarrow{v} &= \lambda_1 \overrightarrow{v}
\end{align}
}

\vspace{-10mm}

\begin{itemize}
	\item \reff{maxOptimisationIV} is due to $\overrightarrow{v}^T \overrightarrow{v} = 1$
	\item \reff{maxOptimisationVII} is due to multiplying $\overrightarrow{v}$ twice to both extremities of the respective equation
\end{itemize}
	
\noindent The final step \reff{maxOptimisationVII} leaves us with a traditional eigenvalue problem which can now be computed in order to obtain the best approximations.


% \subsubsection{Eigendecomposition}

% \begin{itemize}
% 	\item Looks like a good wrapup: \cite{abdi2007eigen}
% \end{itemize}

\clearpage

