With data aggregation rapidly growing in volume over the last few decades, it becomes increasingly more difficult to process gathered data into insightful information.
We are now exploring first cognisances in the field of dimensionality reduction and are then going to summarise underlying theories to be able to better grasp and tackle this problem.

\subsection{The Curse of Dimensionality}

The concept of the \emph{curse of dimensionality} was first introduced by Richard Bellman in 1957. \cite{DynProg}
His book, \emph{Dynamic Programming}, explains a method he had developed to explore more efficient solutions to counteract the increasing complexity in problems facing our day-to-day lives. 
The range of domains where applicable is vast and even covers problems that were impossible to foresee for Richard Bellman.
Most notable to us, this includes data science in the 21st century and falls right into the realm of dimensionality reduction.

Bellman observed that, when considering a larger set of variables, even simple and well-understood problems such as determining the maximum of a given function becomes worrisome.
They face a variety of difficulties of both gross and subtle nature.

The gross issues primarily consist of a finite amount of computational resources available, especially back in the 1950's. 
Having access to the computer systems we have today, they could certainly be considered a computational nirvana for any scientists and mathematicians 65 years ago.
While Bellman phantisised about such possibilities, he proactively considered them and accurately thought of more subtle problems that could potentially arise.
His observations were spot on and are an important factor in why his theories are still relevant as ever nowadays.

\emph{The problem is not to be considered solved in the mathematical sense until the structure of the optimal policy is understood}. \cite{DynProg}
This quote from Richard Bellman eloquently summarises the problem at hand.  
While it appears plausible, that more quantitative measurements would yield more accurate predictions, our extremely complicated world often misleads us.
This results in ourselves neither being able to rigorously understand the problem, nor to improve our predictions of challenges we are unable to analyse.
We need to walk a narrow path between the \emph{Pitfalls of Oversimplification and the Morass of Overcomplication}. \cite{DynProg}

With the premise being described by Bellman in 1957, we can conclude that a large amount of features does not only heavily impede model training, but can additionally result in worse solutions. \cite{HandsOnMLCh8}
Both from a performance and traceability point of view.

\clearpage

\subsection{Necessity}
\subsection{Mathematical background}

To round up the theoretical premises required for this topic, we will become aware of the boundaries of intuitive mathematical concepts which result in highly unintuitive behaviours in high dimensional space, as well as their practical solution approaches.
% Boundaries of euclidian distance in a unit cube and its increasing distance in a seemingly identical position
% Solution approaches through eigenvalues
