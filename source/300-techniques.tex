Being acquainted with the premise, we now possess a coarse understanding of the relevant mathematical scaffold required to understand dimensionality reduction.
Additionally, we became aware of the potential pitfalls that hold true to the subject in general.
Therefore, we are now ready to take a closer look on how to categorise and associate various techniques.


\subsection{Linear vs. non-linear problems}

The utmost category, used to differenciate between various techniques is fortunately mutually exclusive to any method.
Thanks to this caracteristic, we can distinctively classify and associate a given technique.
This allocation, contrary to the following classifications, is the one that considers the domain of the problem.
Thus, it requires special attention by its users since it can easily be used erroneously.
We need to understand the gross patterns in the data set we need to operate on.

The below figure \ref{fig:linearvsnonlinearproblems} illustrates and contrasts the general patterns that we need to identify.\vspace*{4mm}

\renewcommand{\tikzscale}{1.25}
\begin{figure}[h]
	\begin{subfigure}{0.48\textwidth}
	    \caption{Linear problems}
		\input{source/301-linear_problem.tex}
	    \label{subfig:linearproblems}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
	    \caption{Non-linear problems}
		\input{source/302-non_linear_problem.tex}
	    \label{subfig:nonlinearproblems}
	\end{subfigure}
\caption{Linear vs. non-linear problems.}
\label{fig:linearvsnonlinearproblems}
\end{figure}

As we can see in figure \ref{subfig:linearproblems}, a linear problem is given when we are able to identify a straight line to split the data into unambiguous subsets.
Intuitively and, as we will later examine, factually, these types of problem are comparably easy to solve.

A non-linear problem, as pictured in figure \ref{subfig:nonlinearproblems}, already looks more complex on the first glance.
And indeed, we will illuminate the solution approaches to this scenario and elaborate on its comportment.

\clearpage

\subsection{Projection vs. manifold Learning}
% taken from ch 8 in g√©ron's book

\subsection{Greedy vs. exhaustive search algorithms}

Traveling Salesman Problem


\subsection{Linear techniques}

\subsubsection{Principal Component Analysis}
\subsubsection{Singular Value Decomposition}
\subsubsection{Linear Discriminant Analysis}

\subsection{Non-linear techniques}

\subsubsection{Locally Linear Embedding}
\subsubsection{Isomap Embedding}
